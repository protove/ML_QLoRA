2025-11-11 09:55:24 - INFO - ================================================================================
2025-11-11 09:55:24 - INFO - FP16 LoRA Fine-tuning for RTX 3080
2025-11-11 09:55:24 - INFO - TinyLlama-1.1B 모델 학습 스크립트
2025-11-11 09:55:24 - INFO - ================================================================================
2025-11-11 09:55:24 - INFO - 실험 폴더: fp16-lora-tinyllama-3080-final/0001
2025-11-11 09:55:24 - INFO - PyTorch 버전: 2.5.1+cu121
2025-11-11 09:55:24 - INFO - CUDA 사용 가능: True
2025-11-11 09:55:24 - INFO - GPU: NVIDIA GeForce RTX 3080
2025-11-11 09:55:24 - INFO - CUDA 버전: 12.1
2025-11-11 09:55:24 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:24 - INFO - 1. 토크나이저 로드 중...
2025-11-11 09:55:24 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:25 - INFO - ✓ 토크나이저 로드 완료
2025-11-11 09:55:25 - INFO -   모델: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-11-11 09:55:25 - INFO -   Vocabulary 크기: 32000
2025-11-11 09:55:25 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:25 - INFO - 2. FP16 모델 설정 중...
2025-11-11 09:55:25 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:25 - INFO -   - 양자화: 없음 (Full Precision FP16)
2025-11-11 09:55:25 - INFO -   - 계산 타입: float16
2025-11-11 09:55:25 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:25 - INFO - 3. 모델 로드 중... (FP16으로 로드)
2025-11-11 09:55:25 - INFO - --------------------------------------------------------------------------------
`torch_dtype` is deprecated! Use `dtype` instead!
2025-11-11 09:55:38 - INFO - ✓ 모델 로드 완료
2025-11-11 09:55:38 - INFO -   모델 타입: LlamaForCausalLM
2025-11-11 09:55:38 - INFO -   디바이스: cuda:0
2025-11-11 09:55:38 - INFO -   데이터 타입: torch.float16
2025-11-11 09:55:38 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:38 - INFO - 4. LoRA 설정 중... (RTX 3080 최적화)
2025-11-11 09:55:38 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:38 - INFO - ✓ LoRA 설정 완료 (RTX 3080 최적화)
2025-11-11 09:55:38 - INFO -   - 랭크(r): 8 (VRAM 절약 우선)
2025-11-11 09:55:38 - INFO -   - 알파: 16 (r * 2)
2025-11-11 09:55:38 - INFO -   - 드롭아웃: 0.05
2025-11-11 09:55:38 - INFO -   - 대상 모듈 수: 7
2025-11-11 09:55:38 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:38 - INFO - 5. PEFT 모델 준비 중...
2025-11-11 09:55:38 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:39 - INFO - ✓ PEFT 모델 준비 완료
2025-11-11 09:55:39 - INFO -   학습 가능한 파라미터: 6,307,840
2025-11-11 09:55:39 - INFO -   전체 파라미터: 1,106,356,224
2025-11-11 09:55:39 - INFO -   학습 가능 비율: 0.5701%
2025-11-11 09:55:39 - INFO -   GPU 메모리 할당: 2.07 GB
2025-11-11 09:55:39 - INFO -   GPU 메모리 예약: 2.21 GB
2025-11-11 09:55:39 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:39 - INFO - 6. 데이터셋 로드 중...
2025-11-11 09:55:39 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:43 - INFO - ✓ 데이터셋 로드 완료
2025-11-11 09:55:43 - INFO -   - 샘플 수: 15011
2025-11-11 09:55:43 - INFO -   - 컬럼: ['instruction', 'context', 'response', 'category']
2025-11-11 09:55:43 - INFO - 토크나이징 중... (max_length=512로 VRAM 최적화)
Map:   0%|          | 0/15011 [00:00<?, ? examples/s]Map:   1%|          | 118/15011 [00:00<00:13, 1074.21 examples/s]Map:   3%|▎         | 425/15011 [00:00<00:06, 2198.50 examples/s]Map:   5%|▍         | 741/15011 [00:00<00:05, 2623.09 examples/s]Map:   8%|▊         | 1164/15011 [00:00<00:07, 1935.19 examples/s]Map:  10%|▉         | 1495/15011 [00:00<00:05, 2273.64 examples/s]Map:  12%|█▏        | 1814/15011 [00:00<00:05, 2512.35 examples/s]Map:  14%|█▍        | 2176/15011 [00:00<00:05, 2408.84 examples/s]Map:  17%|█▋        | 2514/15011 [00:01<00:04, 2649.84 examples/s]Map:  19%|█▉        | 2856/15011 [00:01<00:04, 2850.83 examples/s]Map:  21%|██        | 3170/15011 [00:01<00:04, 2567.50 examples/s]Map:  23%|██▎       | 3511/15011 [00:01<00:04, 2780.37 examples/s]Map:  26%|██▌       | 3893/15011 [00:01<00:03, 3044.39 examples/s]Map:  29%|██▊       | 4315/15011 [00:01<00:03, 2744.34 examples/s]Map:  31%|███       | 4663/15011 [00:01<00:03, 2919.81 examples/s]Map:  33%|███▎      | 5000/15011 [00:01<00:03, 2755.88 examples/s]Map:  36%|███▌      | 5340/15011 [00:02<00:03, 2911.77 examples/s]Map:  38%|███▊      | 5647/15011 [00:02<00:03, 2949.68 examples/s]Map:  40%|███▉      | 5989/15011 [00:02<00:02, 3076.32 examples/s]Map:  42%|████▏     | 6333/15011 [00:02<00:03, 2734.45 examples/s]Map:  44%|████▍     | 6659/15011 [00:02<00:02, 2866.51 examples/s]Map:  46%|████▋     | 6973/15011 [00:02<00:02, 2938.02 examples/s]Map:  49%|████▉     | 7344/15011 [00:02<00:02, 2690.93 examples/s]Map:  51%|█████     | 7661/15011 [00:02<00:02, 2809.65 examples/s]Map:  53%|█████▎    | 8000/15011 [00:02<00:02, 2621.02 examples/s]Map:  56%|█████▌    | 8351/15011 [00:03<00:02, 2838.11 examples/s]Map:  58%|█████▊    | 8676/15011 [00:03<00:02, 2944.89 examples/s]Map:  60%|█████▉    | 8998/15011 [00:03<00:01, 3016.85 examples/s]Map:  62%|██████▏   | 9342/15011 [00:03<00:02, 2711.76 examples/s]Map:  65%|██████▍   | 9698/15011 [00:03<00:01, 2928.40 examples/s]Map:  68%|██████▊   | 10179/15011 [00:03<00:01, 2793.89 examples/s]Map:  70%|███████   | 10512/15011 [00:03<00:01, 2918.06 examples/s]Map:  72%|███████▏  | 10869/15011 [00:03<00:01, 3083.03 examples/s]Map:  75%|███████▌  | 11331/15011 [00:04<00:01, 2794.57 examples/s]Map:  78%|███████▊  | 11679/15011 [00:04<00:01, 2945.37 examples/s]Map:  80%|███████▉  | 11998/15011 [00:04<00:01, 2995.30 examples/s]Map:  82%|████████▏ | 12326/15011 [00:04<00:01, 2673.40 examples/s]Map:  84%|████████▍ | 12658/15011 [00:04<00:00, 2830.74 examples/s]Map:  87%|████████▋ | 12997/15011 [00:04<00:00, 2967.56 examples/s]Map:  89%|████████▉ | 13324/15011 [00:04<00:00, 2660.06 examples/s]Map:  91%|█████████ | 13669/15011 [00:04<00:00, 2851.22 examples/s]Map:  93%|█████████▎| 14000/15011 [00:05<00:00, 2626.43 examples/s]Map:  95%|█████████▌| 14332/15011 [00:05<00:00, 2785.07 examples/s]Map:  98%|█████████▊| 14708/15011 [00:05<00:00, 3037.13 examples/s]Map: 100%|██████████| 15011/15011 [00:05<00:00, 2667.26 examples/s]
2025-11-11 09:55:49 - INFO - ✓ 토크나이징 완료
2025-11-11 09:55:49 - INFO -   - 최대 길이: 512 토큰 (VRAM 절약을 위한 제한)
2025-11-11 09:55:49 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:49 - INFO - 7. Training Arguments 설정 중... (RTX 3080 최적화)
2025-11-11 09:55:49 - INFO - --------------------------------------------------------------------------------
2025-11-11 09:55:49 - INFO - ✓ Training Arguments 설정 완료 (RTX 3080 최적화)
2025-11-11 09:55:49 - INFO -   - 배치 크기: 1 (VRAM 최소화)
2025-11-11 09:55:49 - INFO -   - Gradient Accumulation: 16
2025-11-11 09:55:49 - INFO -   - 실질 배치 크기: 16
2025-11-11 09:55:49 - INFO -   - 학습률: 0.0001 (FP16 안정화)
2025-11-11 09:55:49 - INFO -   - 스케줄러: cosine
2025-11-11 09:55:49 - INFO -   - 에포크: 5
2025-11-11 09:55:49 - INFO -   - FP16: True (텐서 코어 활용)
2025-11-11 09:55:49 - INFO -   - Gradient Checkpointing: True (VRAM 절약)
2025-11-11 09:55:49 - INFO - ✓ 설정 파일 저장 완료: fp16-lora-tinyllama-3080-final/0001/config.json
2025-11-11 09:55:49 - INFO - ✓ 설정 파일 저장 완료: fp16-lora-tinyllama-3080-final/0001/config.txt
2025-11-11 09:55:49 - INFO - 
--------------------------------------------------------------------------------
2025-11-11 09:55:49 - INFO - 8. Trainer 초기화 중...
2025-11-11 09:55:49 - INFO - --------------------------------------------------------------------------------
The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-11-11 09:55:49 - INFO - ✓ Trainer 초기화 완료
2025-11-11 09:55:49 - INFO - 
================================================================================
2025-11-11 09:55:49 - INFO - 9. 학습 시작 🚀
2025-11-11 09:55:49 - INFO - ================================================================================

2025-11-11 09:55:49 - INFO - ================================================================================
2025-11-11 09:55:49 - INFO - 🚀 학습 시작!
2025-11-11 09:55:49 - INFO - ================================================================================
  0%|          | 0/4695 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2025-11-11 09:55:51 - ERROR - 학습 중 오류 발생: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
  File "/home/protove/vsc/ML_HW/ML_QLoRA/3080/./train_fp16_lora_script.py", line 456, in main
    trainer.train()
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/accelerate/accelerator.py", line 2736, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/protove/anaconda3/envs/QLoRA/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  0%|          | 0/4695 [00:02<?, ?it/s]
