#!/usr/bin/env python3
"""
QLoRA Fine-tuning Script for RTX 3080
Mistral-7B ëª¨ë¸ì„ RTX 3080ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë©° ëª¨ë“  ë¡œê·¸ë¥¼ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path
import gc
import time

import torch
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ì´ ê·¸ë˜í”„ ì €ì¥
import matplotlib.pyplot as plt
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


class ExperimentManager:
    """ì‹¤í—˜ í´ë” ë° ë¡œê¹… ê´€ë¦¬"""
    
    def __init__(self, base_dir="./qlora-mistral-3080-final"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.experiment_dir = self._create_experiment_dir()
        self.logger = self._setup_logging()
        
    def _create_experiment_dir(self):
        """ë‹¤ìŒ ì‹¤í—˜ ë²ˆí˜¸ì˜ í´ë” ìƒì„± (0001, 0002, ...)"""
        existing_dirs = [d for d in self.base_dir.iterdir() if d.is_dir() and d.name.isdigit()]
        
        if existing_dirs:
            max_num = max([int(d.name) for d in existing_dirs])
            next_num = max_num + 1
        else:
            next_num = 1
        
        experiment_dir = self.base_dir / f"{next_num:04d}"
        experiment_dir.mkdir(exist_ok=True)
        
        return experiment_dir
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.experiment_dir / "training.log"
        
        # ë¡œê±° ìƒì„±
        logger = logging.getLogger("QLoRA_Training")
        logger.setLevel(logging.INFO)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§· ì„¤ì •
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def save_config(self, bnb_config, lora_config, training_args):
        """ì„¤ì • íŒŒì¼ë“¤ì„ ë³´ê¸° ì¢‹ê²Œ ì €ì¥"""
        config_file = self.experiment_dir / "config.json"
        
        config_data = {
            "experiment_info": {
                "timestamp": datetime.now().isoformat(),
                "experiment_dir": str(self.experiment_dir),
                "pytorch_version": torch.__version__,
                "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A",
                "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
            },
            "BitsAndBytesConfig": {
                "load_in_4bit": bnb_config.load_in_4bit,
                "bnb_4bit_quant_type": bnb_config.bnb_4bit_quant_type,
                "bnb_4bit_compute_dtype": str(bnb_config.bnb_4bit_compute_dtype),
                "bnb_4bit_use_double_quant": bnb_config.bnb_4bit_use_double_quant,
            },
            "LoraConfig": {
                "r": lora_config.r,
                "lora_alpha": lora_config.lora_alpha,
                "target_modules": lora_config.target_modules,
                "lora_dropout": lora_config.lora_dropout,
                "bias": lora_config.bias,
                "task_type": lora_config.task_type,
            },
            "TrainingArguments": {
                "output_dir": training_args.output_dir,
                "per_device_train_batch_size": training_args.per_device_train_batch_size,
                "gradient_accumulation_steps": training_args.gradient_accumulation_steps,
                "effective_batch_size": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,
                "optim": training_args.optim,
                "learning_rate": training_args.learning_rate,
                "lr_scheduler_type": training_args.lr_scheduler_type,
                "num_train_epochs": training_args.num_train_epochs,
                "fp16": training_args.fp16,
                "logging_steps": training_args.logging_steps,
                "save_strategy": training_args.save_strategy,
                "gradient_checkpointing": training_args.gradient_checkpointing,
            }
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        # í…ìŠ¤íŠ¸ ë²„ì „ë„ ì €ì¥ (ì½ê¸° ì‰½ê²Œ)
        config_txt_file = self.experiment_dir / "config.txt"
        with open(config_txt_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("QLoRA Training Configuration\n")
            f.write("="*80 + "\n\n")
            
            for section, values in config_data.items():
                f.write(f"\n[{section}]\n")
                f.write("-"*80 + "\n")
                for key, value in values.items():
                    f.write(f"{key:30s}: {value}\n")
        
        self.logger.info(f"âœ“ ì„¤ì • íŒŒì¼ ì €ì¥ ì™„ë£Œ: {config_file}")
        self.logger.info(f"âœ“ ì„¤ì • íŒŒì¼ ì €ì¥ ì™„ë£Œ: {config_txt_file}")


class LoggingCallback(TrainerCallback):
    """í•™ìŠµ ê³¼ì •ì„ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•˜ëŠ” ì½œë°±"""
    
    def __init__(self, logger, experiment_dir):
        self.logger = logger
        self.experiment_dir = Path(experiment_dir)
        self.losses = []
        self.steps = []
        self.start_time = None
        
    def on_train_begin(self, args, state, control, **kwargs):
        self.start_time = time.time()
        self.logger.info("="*80)
        self.logger.info("ğŸš€ í•™ìŠµ ì‹œì‘!")
        self.logger.info("="*80)
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if 'loss' in logs:
                self.losses.append(logs['loss'])
                self.steps.append(state.global_step)
            
            # ë¡œê·¸ ë©”ì‹œì§€ í¬ë§·íŒ…
            log_msg = f"Step {state.global_step}/{state.max_steps} | "
            
            if 'loss' in logs:
                log_msg += f"Loss: {logs['loss']:.4f} | "
            if 'learning_rate' in logs:
                log_msg += f"LR: {logs['learning_rate']:.2e} | "
            if 'epoch' in logs:
                log_msg += f"Epoch: {logs['epoch']:.2f} | "
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            if torch.cuda.is_available():
                gpu_mem = torch.cuda.memory_allocated() / 1024**3
                log_msg += f"GPU: {gpu_mem:.2f}GB"
            
            self.logger.info(log_msg)
            
    def on_epoch_end(self, args, state, control, **kwargs):
        self.logger.info(f"âœ“ Epoch {state.epoch} ì™„ë£Œ")
        if self.losses:
            recent_avg = np.mean(self.losses[-10:]) if len(self.losses) >= 10 else np.mean(self.losses)
            self.logger.info(f"  ìµœê·¼ í‰ê·  Loss: {recent_avg:.4f}")
        
    def on_train_end(self, args, state, control, **kwargs):
        total_time = time.time() - self.start_time
        
        self.logger.info("="*80)
        self.logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        self.logger.info("="*80)
        self.logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„ ({total_time/3600:.2f}ì‹œê°„)")
        
        if self.losses:
            self.logger.info(f"ì‹œì‘ Loss: {self.losses[0]:.4f}")
            self.logger.info(f"ìµœì¢… Loss: {self.losses[-1]:.4f}")
            self.logger.info(f"ìµœì†Œ Loss: {min(self.losses):.4f}")
            self.logger.info(f"í‰ê·  Loss: {np.mean(self.losses):.4f}")
            loss_reduction = ((self.losses[0] - self.losses[-1]) / self.losses[0] * 100)
            self.logger.info(f"Loss ê°ì†Œìœ¨: {loss_reduction:.2f}%")
        
        # ìµœì¢… Loss ê·¸ë˜í”„ ì €ì¥
        self.save_loss_plot()
        
    def save_loss_plot(self):
        """Loss ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.losses:
            self.logger.warning("ì €ì¥í•  í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        plt.figure(figsize=(12, 6))
        plt.plot(self.steps, self.losses, 'b-', linewidth=2, label='Training Loss')
        plt.xlabel('Steps', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('Training Loss Curve', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        
        plot_file = self.experiment_dir / "training_loss.png"
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"âœ“ í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: {plot_file}")
        
        # Loss ë°ì´í„°ë„ CSVë¡œ ì €ì¥
        loss_data_file = self.experiment_dir / "training_loss.csv"
        with open(loss_data_file, 'w') as f:
            f.write("step,loss\n")
            for step, loss in zip(self.steps, self.losses):
                f.write(f"{step},{loss}\n")
        
        self.logger.info(f"âœ“ Loss ë°ì´í„° ì €ì¥ ì™„ë£Œ: {loss_data_file}")


def format_instruction(sample):
    """í”„ë¡¬í”„íŠ¸ í˜•ì‹í™” í•¨ìˆ˜"""
    return f"""### Instruction:
{sample['instruction']}

### Context:
{sample['context']}

### Response:
{sample['response']}
"""


def data_collator(data):
    """ë°ì´í„° ì½œë ˆì´í„°"""
    return {
        'input_ids': torch.stack([torch.tensor(f['input_ids']) for f in data]),
        'attention_mask': torch.stack([torch.tensor(f['attention_mask']) for f in data]),
        'labels': torch.stack([torch.tensor(f['input_ids']) for f in data])
    }


def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # ì‹¤í—˜ ê´€ë¦¬ì ì´ˆê¸°í™”
    exp_manager = ExperimentManager()
    logger = exp_manager.logger
    
    logger.info("="*80)
    logger.info("QLoRA Fine-tuning for RTX 3080")
    logger.info("Mistral-7B ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    logger.info("="*80)
    logger.info(f"ì‹¤í—˜ í´ë”: {exp_manager.experiment_dir}")
    logger.info(f"PyTorch ë²„ì „: {torch.__version__}")
    logger.info(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"CUDA ë²„ì „: {torch.version.cuda}")
    else:
        logger.error("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 1. ëª¨ë¸ ID ë° í† í¬ë‚˜ì´ì € ì„¤ì •
    logger.info("\n" + "-"*80)
    logger.info("1. í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    logger.info("-"*80)
    
    model_id = "mistralai/Mistral-7B-v0.1"
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    logger.info(f"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    logger.info(f"  ëª¨ë¸: {model_id}")
    logger.info(f"  Vocabulary í¬ê¸°: {len(tokenizer)}")
    
    # 2. QLoRA ì„¤ì •
    logger.info("\n" + "-"*80)
    logger.info("2. QLoRA ì„¤ì • ì¤‘...")
    logger.info("-"*80)
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    logger.info("âœ“ QLoRA ì„¤ì • ì™„ë£Œ")
    logger.info("  - 4ë¹„íŠ¸ ì–‘ìí™”: í™œì„±í™”")
    logger.info("  - ì–‘ìí™” íƒ€ì…: NF4")
    logger.info("  - ê³„ì‚° íƒ€ì…: bfloat16")
    logger.info("  - ì´ì¤‘ ì–‘ìí™”: í™œì„±í™”")
    
    # 3. ëª¨ë¸ ë¡œë“œ
    logger.info("\n" + "-"*80)
    logger.info("3. ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    logger.info("-"*80)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    logger.info("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    logger.info(f"  ëª¨ë¸ íƒ€ì…: {type(model).__name__}")
    logger.info(f"  ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
    
    # 4. LoRA ì„¤ì •
    logger.info("\n" + "-"*80)
    logger.info("4. LoRA ì„¤ì • ì¤‘...")
    logger.info("-"*80)
    
    peft_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    logger.info("âœ“ LoRA ì„¤ì • ì™„ë£Œ")
    logger.info(f"  - ë­í¬(r): {peft_config.r}")
    logger.info(f"  - ì•ŒíŒŒ: {peft_config.lora_alpha}")
    logger.info(f"  - ë“œë¡­ì•„ì›ƒ: {peft_config.lora_dropout}")
    logger.info(f"  - ëŒ€ìƒ ëª¨ë“ˆ ìˆ˜: {len(peft_config.target_modules)}")
    
    # 5. PEFT ëª¨ë¸ ì¤€ë¹„
    logger.info("\n" + "-"*80)
    logger.info("5. PEFT ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
    logger.info("-"*80)
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, peft_config)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    trainable_percent = 100 * trainable_params / all_params
    
    logger.info("âœ“ PEFT ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    logger.info(f"  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    logger.info(f"  ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,}")
    logger.info(f"  í•™ìŠµ ê°€ëŠ¥ ë¹„ìœ¨: {trainable_percent:.4f}%")
    
    if torch.cuda.is_available():
        logger.info(f"  GPU ë©”ëª¨ë¦¬ í• ë‹¹: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        logger.info(f"  GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
    
    # 6. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    logger.info("\n" + "-"*80)
    logger.info("6. ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    logger.info("-"*80)
    
    data = load_dataset("databricks/databricks-dolly-15k", split="train").shuffle()
    
    logger.info(f"âœ“ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    logger.info(f"  - ìƒ˜í”Œ ìˆ˜: {len(data)}")
    logger.info(f"  - ì»¬ëŸ¼: {data.column_names}")
    
    # í† í¬ë‚˜ì´ì§•
    logger.info("í† í¬ë‚˜ì´ì§• ì¤‘...")
    tokenized_data = data.map(
        lambda p: tokenizer(format_instruction(p), truncation=True, max_length=512, padding="max_length"),
        remove_columns=data.column_names
    )
    
    logger.info("âœ“ í† í¬ë‚˜ì´ì§• ì™„ë£Œ")
    logger.info(f"  - ìµœëŒ€ ê¸¸ì´: 512 í† í°")
    
    # 7. TrainingArguments ì„¤ì •
    logger.info("\n" + "-"*80)
    logger.info("7. Training Arguments ì„¤ì • ì¤‘...")
    logger.info("-"*80)
    
    training_args = TrainingArguments(
        output_dir=str(exp_manager.experiment_dir / "checkpoints"),
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        optim="paged_adamw_32bit",
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        num_train_epochs=5,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
        gradient_checkpointing=True,
    )
    
    logger.info("âœ“ Training Arguments ì„¤ì • ì™„ë£Œ")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
    logger.info(f"  - Gradient Accumulation: {training_args.gradient_accumulation_steps}")
    logger.info(f"  - ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    logger.info(f"  - í•™ìŠµë¥ : {training_args.learning_rate}")
    logger.info(f"  - ìŠ¤ì¼€ì¤„ëŸ¬: {training_args.lr_scheduler_type}")
    logger.info(f"  - ì—í¬í¬: {training_args.num_train_epochs}")
    
    # ì„¤ì • ì €ì¥
    exp_manager.save_config(bnb_config, peft_config, training_args)
    
    # 8. Trainer ì´ˆê¸°í™”
    logger.info("\n" + "-"*80)
    logger.info("8. Trainer ì´ˆê¸°í™” ì¤‘...")
    logger.info("-"*80)
    
    logging_callback = LoggingCallback(logger, exp_manager.experiment_dir)
    
    trainer = Trainer(
        model=model,
        train_dataset=tokenized_data,
        args=training_args,
        data_collator=data_collator,
        callbacks=[logging_callback]
    )
    
    logger.info("âœ“ Trainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 9. í•™ìŠµ ì‹œì‘
    logger.info("\n" + "="*80)
    logger.info("9. í•™ìŠµ ì‹œì‘ ğŸš€")
    logger.info("="*80 + "\n")
    
    try:
        trainer.train()
    except Exception as e:
        logger.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return
    
    # 10. ëª¨ë¸ ì €ì¥
    logger.info("\n" + "-"*80)
    logger.info("10. ëª¨ë¸ ì €ì¥ ì¤‘...")
    logger.info("-"*80)
    
    model_save_dir = exp_manager.experiment_dir / "model"
    model_save_dir.mkdir(exist_ok=True)
    
    model.save_pretrained(model_save_dir)
    tokenizer.save_pretrained(model_save_dir)
    
    logger.info(f"âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_dir}")
    
    # 11. ìµœì¢… ìš”ì•½
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    logger.info("="*80)
    
    if logging_callback.losses:
        logger.info(f"ì´ Step ìˆ˜: {len(logging_callback.steps)}")
        logger.info(f"ì‹œì‘ Loss: {logging_callback.losses[0]:.4f}")
        logger.info(f"ìµœì¢… Loss: {logging_callback.losses[-1]:.4f}")
        logger.info(f"ìµœì†Œ Loss: {min(logging_callback.losses):.4f}")
        logger.info(f"í‰ê·  Loss: {np.mean(logging_callback.losses):.4f}")
        loss_reduction = ((logging_callback.losses[0] - logging_callback.losses[-1]) / logging_callback.losses[0] * 100)
        logger.info(f"Loss ê°ì†Œìœ¨: {loss_reduction:.2f}%")
    
    # 12. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    logger.info("\n" + "-"*80)
    logger.info("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    logger.info("-"*80)
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        logger.info(f"âœ“ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        logger.info(f"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    logger.info(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {exp_manager.experiment_dir}")
    logger.info("="*80)


if __name__ == "__main__":
    main()
