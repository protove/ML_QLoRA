# QLoRA Training Script for RTX 3080

ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ê³  ë¡œê·¸ë¥¼ ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” QLoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

- `train_qlora_script.py` - ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- `run_training.sh` - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `check_status.sh` - í•™ìŠµ ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

## ğŸš€ ì‚¬ìš©ë²•

### 1. í•™ìŠµ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)

```bash
bash run_training.sh
```

ë˜ëŠ”

```bash
nohup python3 train_qlora_script.py > training_background.log 2>&1 &
```

### 2. í•™ìŠµ ìƒíƒœ í™•ì¸

```bash
bash check_status.sh
```

### 3. ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸

```bash
# ë°±ê·¸ë¼ìš´ë“œ ë¡œê·¸
tail -f training_background.log

# ë˜ëŠ” ìµœì‹  ì‹¤í—˜ í´ë”ì˜ ë¡œê·¸
tail -f qlora-mistral-3080-final/0001/training.log
```

### 4. í•™ìŠµ ì¤‘ì§€

```bash
# PID í™•ì¸
cat training.pid

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill $(cat training.pid)
```

## ğŸ“Š ê²°ê³¼ ì €ì¥ êµ¬ì¡°

í•™ìŠµì´ ì™„ë£Œë˜ë©´ `qlora-mistral-3080-final/` í´ë”ì— ìë™ìœ¼ë¡œ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ í´ë”ê°€ ìƒì„±ë©ë‹ˆë‹¤:

```
qlora-mistral-3080-final/
â”œâ”€â”€ 0001/                           # ì²« ë²ˆì§¸ ì‹¤í—˜
â”‚   â”œâ”€â”€ config.json                 # ì„¤ì • ì •ë³´ (JSON)
â”‚   â”œâ”€â”€ config.txt                  # ì„¤ì • ì •ë³´ (í…ìŠ¤íŠ¸)
â”‚   â”œâ”€â”€ training.log                # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ training_loss.png           # Loss ê·¸ë˜í”„
â”‚   â”œâ”€â”€ training_loss.csv           # Loss ë°ì´í„°
â”‚   â”œâ”€â”€ checkpoints/                # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â”‚   â””â”€â”€ checkpoint-xxx/
â”‚   â””â”€â”€ model/                      # ìµœì¢… í•™ìŠµ ëª¨ë¸
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â”œâ”€â”€ adapter_model.safetensors
â”‚       â””â”€â”€ ...
â”œâ”€â”€ 0002/                           # ë‘ ë²ˆì§¸ ì‹¤í—˜
â”‚   â””â”€â”€ ...
â””â”€â”€ 0003/                           # ì„¸ ë²ˆì§¸ ì‹¤í—˜
    â””â”€â”€ ...
```

## ğŸ“ ì €ì¥ë˜ëŠ” íŒŒì¼ë“¤

### 1. `config.json` / `config.txt`
í•™ìŠµì— ì‚¬ìš©ëœ ëª¨ë“  ì„¤ì •ê°’ì´ ì €ì¥ë©ë‹ˆë‹¤:
- **BitsAndBytesConfig** - ì–‘ìí™” ì„¤ì •
- **LoraConfig** - LoRA íŒŒë¼ë¯¸í„°
- **TrainingArguments** - í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- **ì‹¤í—˜ ì •ë³´** - íƒ€ì„ìŠ¤íƒ¬í”„, GPU ì •ë³´ ë“±

### 2. `training.log`
í•™ìŠµ ê³¼ì •ì˜ ëª¨ë“  ë¡œê·¸:
- ê° ìŠ¤í…ì˜ Loss
- Learning Rate ë³€í™”
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ì—í¬í¬ë³„ ì§„í–‰ ìƒí™©
- ìµœì¢… í†µê³„

### 3. `training_loss.png`
í•™ìŠµ Loss ê³¡ì„  ê·¸ë˜í”„

### 4. `training_loss.csv`
ê° ìŠ¤í…ì˜ Loss ê°’ (CSV í˜•ì‹)

### 5. `model/`
í•™ìŠµ ì™„ë£Œëœ LoRA ì–´ëŒ‘í„° ëª¨ë¸

## ğŸ” ì£¼ìš” ê¸°ëŠ¥

### âœ… ìë™ í´ë” ë²ˆí˜¸ ë§¤ê¸°ê¸°
- ê¸°ì¡´ í´ë” í™•ì¸ í›„ ë‹¤ìŒ ë²ˆí˜¸ë¡œ ìë™ ìƒì„±
- 0001, 0002, 0003... í˜•ì‹

### âœ… ì²´ê³„ì ì¸ ë¡œê¹…
- íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ ë¡œê¹…
- íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
- í•™ìŠµ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ê¸°ë¡

### âœ… ì„¤ì • ìë™ ì €ì¥
- JSONê³¼ í…ìŠ¤íŠ¸ ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
- ì¬í˜„ ê°€ëŠ¥ì„± ë³´ì¥

### âœ… ì‹œê°í™” ìë™ ì €ì¥
- Loss ê·¸ë˜í”„ PNG íŒŒì¼ë¡œ ì €ì¥
- CSV ë°ì´í„°ë¡œë„ ì œê³µ

### âœ… GPU ë©”ëª¨ë¦¬ ìµœì í™”
- 4ë¹„íŠ¸ ì–‘ìí™” (QLoRA)
- Gradient Checkpointing
- RTX 3080 ìµœì í™” ì„¤ì •

## âš™ï¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

`train_qlora_script.py`ì˜ `main()` í•¨ìˆ˜ì—ì„œ ë‹¤ìŒ ê°’ë“¤ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### LoRA ì„¤ì •
```python
peft_config = LoraConfig(
    r=8,                    # ë­í¬ (â†“ ë©”ëª¨ë¦¬ ì ˆì•½, â†“ ì„±ëŠ¥)
    lora_alpha=16,          # ì•ŒíŒŒ (ë³´í†µ r * 2)
    lora_dropout=0.05,      # ë“œë¡­ì•„ì›ƒ
    # ...
)
```

### í•™ìŠµ ì„¤ì •
```python
training_args = TrainingArguments(
    per_device_train_batch_size=1,      # ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=16,     # Gradient ëˆ„ì 
    learning_rate=2e-4,                 # í•™ìŠµë¥ 
    num_train_epochs=5,                 # ì—í¬í¬ ìˆ˜
    # ...
)
```

## ğŸ“ˆ í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
```bash
# ìµœì‹  ì‹¤í—˜ í´ë”ì˜ ë¡œê·¸ í™•ì¸
tail -f qlora-mistral-3080-final/$(ls -t qlora-mistral-3080-final/ | head -1)/training.log
```

### GPU ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
watch -n 1 nvidia-smi
```

### í”„ë¡œì„¸ìŠ¤ í™•ì¸
```bash
ps aux | grep train_qlora_script.py
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### CUDA Out of Memory
- `per_device_train_batch_size`ë¥¼ 1ë¡œ ìœ ì§€
- `gradient_accumulation_steps` ì¦ê°€
- `r` (LoRA rank) ê°ì†Œ (ì˜ˆ: 8 â†’ 4)

### í•™ìŠµ ì†ë„ ëŠë¦¼
- `gradient_checkpointing=False` (ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ìˆë‹¤ë©´)
- ë°ì´í„°ì…‹ í¬ê¸° ì¶•ì†Œ

### í”„ë¡œì„¸ìŠ¤ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¢…ë£Œë¨
- `training_background.log` í™•ì¸
- GPU ë“œë¼ì´ë²„ í™•ì¸
- CUDA ë©”ëª¨ë¦¬ ë¶€ì¡± í™•ì¸

## ğŸ“Š ì‹¤í—˜ ë¹„êµ

ì—¬ëŸ¬ ì‹¤í—˜ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ë ¤ë©´:

```bash
# ê° ì‹¤í—˜ì˜ config.json í™•ì¸
cat qlora-mistral-3080-final/0001/config.json
cat qlora-mistral-3080-final/0002/config.json

# Loss ê·¸ë˜í”„ ë¹„êµ
eog qlora-mistral-3080-final/*/training_loss.png
```

## ğŸ’¡ íŒ

1. **ì¥ì‹œê°„ í•™ìŠµ ì‹œ** SSH ì„¸ì…˜ì´ ëŠì–´ì ¸ë„ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤ (nohup ì‚¬ìš©)
2. **ì—¬ëŸ¬ ì‹¤í—˜ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥** (GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´)
3. **ë¡œê·¸ íŒŒì¼ë¡œ í•™ìŠµ ì™„ë£Œ í™•ì¸** - "ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!" ë©”ì‹œì§€ í™•ì¸
4. **ì‹¤í—˜ ë²ˆí˜¸ë¡œ ë²„ì „ ê´€ë¦¬** - Gitì— ì½”ë“œë§Œ ì˜¬ë¦¬ê³  ëª¨ë¸ì€ ë¡œì»¬ì— ë³´ê´€

## ğŸ“¦ ìš”êµ¬ ì‚¬í•­

```bash
pip install torch transformers datasets peft bitsandbytes accelerate matplotlib
```

ë˜ëŠ”

```bash
pip install -r requirements_3080.txt
```

---

**Happy Training! ğŸš€**
