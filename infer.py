import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# âœ… LoRA ëª¨ë¸ ê²½ë¡œ
MODEL_DIR = "./tinyllama-lora-mac"

def load_model():
    print("ğŸ“Œ Loading base model + LoRA adapter...")

    # ğŸ”¥ Base Model Load (MPS ì ìš©)
    base_model = AutoModelForCausalLM.from_pretrained(
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        torch_dtype=torch.float16,
        device_map={"": "mps"},
    )

    # ğŸ”¥ Merge LoRA Adapter
    model = PeftModel.from_pretrained(base_model, MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

    print("âœ… LoRA adapter loaded successfully!\n")
    return model, tokenizer


def generate_answer(model, tokenizer, user_input):
    # ğŸ“ Alpaca-style formatting optional (but simple mode here)
    input_ids = tokenizer(user_input, return_tensors="pt").input_ids.to("mps")

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            max_new_tokens=200,
            do_sample=True,          # âœ… ìƒ˜í”Œë§ í™œì„±í™”
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,  # âœ… ë”°ë¼ì“°ê¸° ê°ì†Œ
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)


if __name__ == "__main__":
    model, tokenizer = load_model()
    print("ğŸ§  TinyLlama + LoRA Inference Ready!")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥\n")

    while True:
        user_input = input("ğŸ’¬ You: ")
        if user_input.lower() == "quit":
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í–ˆì–´!")
            break

        answer = generate_answer(model, tokenizer, user_input)
        print(f"ğŸ¤– Model: {answer}\n")
