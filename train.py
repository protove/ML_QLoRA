import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# -----------------------------
# 0. MPS(Apple Silicon GPU) í™•ì¸
# -----------------------------
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"âœ… Training Device: {device}")

# -----------------------------
# 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ (TinyLlama 1.1B)
# -----------------------------
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print("ğŸ“Œ Loading TinyLlama model...")
tokenizer = AutoTokenizer.from_pretrained(model_id)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# MPSì—ì„œëŠ” bitsandbytes ì‚¬ìš© ë¶ˆê°€ â†’ FP16 + MPS ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map={"": device}
)

# -----------------------------
# 2. LoRA ì„¤ì •
# -----------------------------
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # TinyLlama í˜¸í™˜ ëª¨ë“ˆë§Œ ì ìš©
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# k-bit í›ˆë ¨ ì¤€ë¹„ (Macì—ì„œë„ í•„ìš”)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

print("âœ… PEFT + LoRA Ready!")
model.print_trainable_parameters()

# -----------------------------
# 3. ë°ì´í„° ë¡œë“œ (Mini Alpaca 3k)
# -----------------------------
print("ğŸ“‚ Loading dataset...")
dataset = load_dataset("tatsu-lab/alpaca", split="train[:3000]")  # 3k

def format_prompt(sample):
    return f"""### Instruction:
{sample['instruction']}

### Input:
{sample['input']}

### Output:
{sample['output']}
"""

def tokenize(example):
    text = format_prompt(example)
    tokens = tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_dataset = dataset.map(tokenize, batched=False)

# -----------------------------
# 4. TrainingArguments ì„¤ì •
# -----------------------------
training_args = TrainingArguments(
    output_dir="./tinyllama_lora_output_v2",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    learning_rate=2e-4,

    fp16=False,     # â— MPSëŠ” fp16 ëŒ€ì‹  False ë˜ëŠ” bf16 ì‚¬ìš©
    bf16=False,     # MPSì—ì„œ bf16 ë¯¸ì§€ì› â†’ False ìœ ì§€

    logging_dir="./logs",      # âœ… ë¡œê·¸ ì €ì¥ í´ë”
    logging_steps=10,          # âœ… 10 stepë§ˆë‹¤ loss ê¸°ë¡
    save_steps=375,            # âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_total_limit=3,        # âœ… ìµœê·¼ 3ê°œë§Œ ì €ì¥
    report_to=["tensorboard"], # âœ… í…ì„œë³´ë“œ ë¡œê·¸ ê¸°ë¡

    optim="adamw_torch",          # â­ MPSì—ì„œ ê°€ì¥ ì•ˆì •ì 
    lr_scheduler_type="cosine",   # â­ loss ì•ˆì •í™”
    warmup_ratio=0.03,            # â­ ì´ˆë°˜ loss í­ì£¼ ë°©ì§€
)

# -----------------------------
# 5. Trainer ì‹¤í–‰
# -----------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

print("ğŸš€ Training Started...")
trainer.train()

# -----------------------------
# 6. í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ì €ì¥
# -----------------------------
save_path = "./tinyllama-lora-mac"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"ğŸ‰ Training Completed! LoRA adapter saved at: {save_path}")
